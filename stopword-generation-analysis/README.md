# Datasset for analysing using Wikipedia to generate stopwords
An analysis on best and most effective way of [using a Wikipedia-site to generate a list of stopwords](https://medium.com/norch/an-analysis-on-using-wikipedia-to-generate-stopword-lists-aac30bd38f9c) for a language.


What size of document corpus has to say for quality
* [100 vs 1000 documents](https://github.com/eklem/dataset-misc/commit/85f318a7367d3574b9976cbe3261a8d5a659ae93)
* [1000 vs 10000 documents](https://github.com/eklem/dataset-misc/commit/64762954cacc3132e2e80337f99ac6f81a0add08)
* [10000 vs 40000 documents](https://github.com/eklem/dataset-misc/commit/49b9559401c2f5e4ba9d9dc67cdb6d640cc34803)
* [40000 vs 108501 documents](https://github.com/eklem/dataset-misc/commit/4de5b282d1e646025fa39eca59d6ad2a6a254554)


What choosing/selecting the right subset has to say for quality, possibly making the process more effective
* [10000 selected vs 108501 documents](https://github.com/eklem/dataset-misc/commit/b12c1a7c1b7c9e8001399c771d906e70b7cb9f19)

